{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dislib tutorial\n",
    "\n",
    "This tutorial will show the basics of using [dislib](https://dislib.bsc.es).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Apart from dislib, this notebook requires [PyCOMPSs](https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/).\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "\n",
    "First, we need to start an interactive PyCOMPSs session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycompss.interactive as ipycompss\n",
    "ipycompss.start(graph=True, monitor=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import dislib and we are all set to start working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dislib as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed arrays\n",
    "\n",
    "The main data structure in dislib is the distributed array (or ds-array). These arrays are a distributed representation of a 2-dimensional array that can be operated as a regular Python object. Usually, rows in the array represent samples, while columns represent features.\n",
    "\n",
    "To create a random array we can run the following NumPy-like command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.random_array(shape=(500, 500), block_size=(100, 100))\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `x` is a 500x500 ds-array of random numbers stored in blocks of 100x100 elements. Note that `x` is not stored in memory. Instead, `random_array` generates the contents of the array in tasks that are usually executed remotely. This allows the creation of really big arrays.\n",
    "\n",
    "The content of `x` is a list of `Futures` that represent the actual data (wherever it is stored).\n",
    "\n",
    "To see this, we can access the `_blocks` field of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x._blocks[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`block_size` is useful to control the granularity of dislib algorithms.\n",
    "\n",
    "To retrieve the actual contents of `x`, we use `collect`, which synchronizes the data and returns the equivalent NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating ds-arrays is using array-like structures like NumPy arrays or lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ds.array([[1, 2, 3], [4, 5, 6]], block_size=(1, 3))\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed arrays can also store sparse data in CSR format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "sp = csr_matrix([[0, 0, 1], [1, 0, 1]])\n",
    "x_sp = ds.array(sp, block_size=(1, 3))\n",
    "x_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `collect` returns a CSR matrix as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "A typical way of creating ds-arrays is to load data from disk. Dislib currently supports reading data in CSV and SVMLight formats like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ds.load_svmlight_file(\"../tests/files/libsvm/1\", block_size=(20, 100), n_features=780, store_sparse=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = ds.load_txt_file(\"../tests/files/csv/1\", block_size=(500, 122))\n",
    "print(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "Similar to NumPy, ds-arrays support the following types of slicing:\n",
    "\n",
    "(Note that slicing a ds-array creates a new ds-array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.random_array((50, 50), (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a single element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2, 3].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a set of rows or a set of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consecutive rows\n",
    "print(x[10:20])\n",
    "\n",
    "# Consecutive columns\n",
    "print(x[:, 10:20])\n",
    "\n",
    "# Non consecutive rows\n",
    "print(x[[3, 7, 22]])\n",
    "\n",
    "# Non consecutive columns\n",
    "print(x[:, [5, 9, 48]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get any set of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0:5, 40:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "\n",
    "Apart from this, ds-arrays also provide other useful operations like `transpose` and `mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean(axis=0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.transpose().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with dislib\n",
    "\n",
    "Dislib provides an estimator-based API very similar to [scikit-learn](https://scikit-learn.org/stable/). An estimator is anything that learns from data. To illustrate how an estimator works, let's first generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "x_np, y = make_blobs(n_samples=1500, random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_np` and `y` are random samples and labels. Samples are vectors and labels are numbers that represent the category of each sample. In this example, we are going to run K-means clustering, which is useful to understand **unlabeled** data, and thus, we will not use `y`. \n",
    "\n",
    "Since the samples in `x_np` are 2-dimensional, we can plot them and see that there are 3 clusters in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x_np[:, 0], x_np[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how good is K-means in detecting these three clusters.\n",
    "\n",
    "To use dislib, we first need to convert `x` to a ds-array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.array(x_np, block_size=(300, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a K-means instance. In K-means, we need to define the number of clusters in our data. Since this example is simple, the obvious value here is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to fit our estimator with training data (i.e., `x_ds`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit method does the main part of the computational work. In the case of K-means, fitting means finding the optimal cluster centers given some training data. K-means will find as many cluster centers as specified in the constructor (i.e., 3). \n",
    "\n",
    "We can check the computed centers like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an estimator is fitted, we can compute the labels of unlabeled data. For example, we can compute labels for `x` using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = km.predict(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` is a ds-array of predicted labels for `x`. The prediction process depends on the cluster centers computed in the `fit` step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the samples in `x`, using the predicted labels as colors (we also plot the cluster centers in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = km.centers\n",
    "\n",
    "# set the color of each sample to the predicted label\n",
    "plt.scatter(x_np[:, 0], x_np[:, 1], c=y_pred.collect())\n",
    "\n",
    "# plot the computed centers in red\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to call `y_pred.collect()` to retrieve the actual labels and plot them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DBSCAN\n",
    "\n",
    "K-means is a simple yet effective clustering method. However, K-means has a major drawback: the number of clusters needs to be defined beforehand. This is no always possible, and other clustering methods have attempted to address this limitation.\n",
    "\n",
    "An example is DBSCAN, which is a density based clustering algorithm. In DBSCAN, users define density using two parameters: `eps` and `min_samples`. The algorithm then finds an arbitrary number of clusters based on these two parameters.\n",
    "\n",
    "Your task now is to experiment with different `eps` and `min_samples` values to see how DBSCAN performs with the blob data!\n",
    "\n",
    "**WARNING:** DBSCAN works a bit different to K-means. You can find its API reference [here](https://dislib.bsc.es/en/stable/dislib.cluster.dbscan.html#dislib.cluster.dbscan.base.DBSCAN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.cluster import DBSCAN\n",
    "\n",
    "# fill in the values for eps and min_samples\n",
    "dbscan = DBSCAN(eps=1, min_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict the labels for x\n",
    "y_pred = dbscan.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can plot the results with the following code (assuming predicted labels are in `y_pred`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the color of each sample to the predicted label\n",
    "plt.scatter(x_np[:, 0], x_np[:, 1], c=y_pred.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "x_np, _ = make_circles(n_samples=1500, factor=.5, noise=.05, random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use K-means and DBSCAN to cluster the data in x_np. Which algorithm performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ds-array\n",
    "x = ds.array(x_np, block_size=(300, 2))\n",
    "\n",
    "# Create estimators\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "\n",
    "# Fit and predict labels\n",
    "y_dbscan = dbscan.fit_predict(x)\n",
    "y_km = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to plot the results\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "ax.title.set_text(\"DBSCAN\")\n",
    "ax.scatter(x_np[:, 0], x_np[:, 1], c=y_dbscan.collect())\n",
    "ax = plt.subplot(122)\n",
    "ax.title.set_text(\"K-means\")\n",
    "ax.scatter(x_np[:, 0], x_np[:, 1], c=y_km.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now we will solve an exercise using the digits data set from scikit-learn. Samples in this data set represent images of handwritten digits (0 to 9), where each feature represents a pixel in the image.\n",
    "\n",
    "First, we load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "x_np = digits.data\n",
    "y = digits.target.reshape(-1, 1)\n",
    "\n",
    "x_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_np` contains 1797 samples of 64 features, and `y` contains the labels, which in this case is the handwritten number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = x_np[23]\n",
    "digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see actually see the digit by reshaping the vector and plotting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = digit.reshape((8,8))\n",
    "plt.imshow(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the corresponding label should be a 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the original data set has 10 different labels, we can simplify the problem by converting it into a binary classification problem, where odd numbers have label=0 and even numbers have label=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y%2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `y[23]` should be 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is different from clustering in that labels are also used for the fitting process. Once a classifier is fitted, we can use it to label unlabeled data.\n",
    "\n",
    "To simulate having labeled (training) and unlabeled (test) data, we split the digits data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_np, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_train` and `x_test` now contain 80% and 20% of the samples in `x` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use `CascadeSVM` and `RandomForestClassifier` to classify the digits data, and get the accuracy obtained!\n",
    "\n",
    "### Hints:\n",
    "\n",
    "- You can find dislib's API reference [here](https://dislib.bsc.es/en/stable/api-reference.html).\n",
    "- Remember to convert data to ds-arrays before passing them to the classifiers.\n",
    "- Do not worry too much about the classifiers' parameters, you can use the default values.\n",
    "- Use the train data to fit the estimators, and the test data to check the accuracy.\n",
    "- Accuracy can be obtained using the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping is needed because of reasons\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# convert your data to ds-arrays\n",
    "x_ds_train = ds.array(x_train, block_size=(300, 64))\n",
    "y_ds_train = ds.array(y_train, block_size=(300, 1))\n",
    "\n",
    "x_ds_test = ds.array(x_test, (300, 64))\n",
    "y_ds_test = ds.array(y_test, (300, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.classification import CascadeSVM\n",
    "\n",
    "# create CascadeSVM estimator\n",
    "csvm = CascadeSVM(gamma=0.1)\n",
    "\n",
    "\n",
    "# fit the estimator with training data\n",
    "csvm.fit(x_ds_train, y_ds_train)\n",
    "\n",
    "\n",
    "# print the accuracy on the test data\n",
    "score = csvm.score(x_ds_test, y_ds_test)\n",
    "\n",
    "from pycompss.api.api import compss_wait_on\n",
    "print(compss_wait_on(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same as above using the RandomForestClassifier :)\n",
    "\n",
    "from dislib.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(x_ds_train, y_ds_train)\n",
    "\n",
    "print(compss_wait_on(rf.score(x_ds_test, y_ds_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which classifier gets better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Classifiers' performance is highly sensitive to the initialization parameters (or hyperparameters), and it is difficult to know which parameters are optimal beforehand. Thankfully, there are hyperparameter optimization techniques that allow us to find good parameters for our classification problem.\n",
    "\n",
    "One of these techniques is grid search with cross-validation. This model selection algorithm performs an exhaustive search on a predefined set of hyperparameters to find the optimal ones.\n",
    "\n",
    "Try to improve the score of the CascadeSVM classifier using grid search! You can find GridSearchCV reference [here](https://dislib.bsc.es/en/latest/dislib.model_selection.html#dislib.model_selection.GridSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.model_selection import GridSearchCV\n",
    "\n",
    "# hyperparameter search space\n",
    "params = {\"gamma\" : (0.1, 0.01, 0.0001), \"c\" : (1, 10, 100)}\n",
    "\n",
    "csvm = CascadeSVM()\n",
    "\n",
    "# use grid search with your training data (it might take a while, be patient)\n",
    "searcher = GridSearchCV(csvm, params)\n",
    "searcher.fit(x_ds_train, y_ds_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to print the results in a nice way\n",
    "import pandas as pd\n",
    "pd.DataFrame(searcher.cv_results_)[[\"params\", \"mean_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try the optimal parameters with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal parameters here\n",
    "csvm = CascadeSVM(gamma=0.0001, c=100)\n",
    "\n",
    "csvm.fit(x_ds_train, y_ds_train)\n",
    "\n",
    "from pycompss.api.api import compss_wait_on\n",
    "compss_wait_on(csvm.score(x_ds_test, y_ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the session\n",
    "\n",
    "To finish the session, we need to stop PyCOMPSs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
